{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5301cf62-cd98-43ac-bd81-af48467f7cf9",
   "metadata": {},
   "source": [
    "# Creating a PD model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ec142e-e94d-43eb-b08e-eea935ae7d6a",
   "metadata": {},
   "source": [
    "- What is Probability of Default?\n",
    "\n",
    "- The probability of default (PD) is the probability of a borrower or debtor defaulting on loan repayments. Within financial markets, an assetâ€™s probability of default is the probability that the asset yields no return to its holder over its lifetime and the asset price goes to zero. Investors use the probability of default to calculate the expected loss from an investment.\n",
    "\n",
    "\n",
    "- What is PD Model?\n",
    "- A probability of default (PD) model is a statistical model used to estimate the likelihood that a borrower will default on their loan obligations within a given time frame. There are various approaches to building PD models, including traditional statistical methods and machine learning algorithms. Below steps involved in developing a PD model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb4e7de-6ecb-4a27-8c39-b077cb8aaa86",
   "metadata": {},
   "source": [
    "## Step-1 \n",
    "### Data Collection: The bank gathers data on past borrowers, including relevant variables such as age, income, credit score, outstanding debt, and default status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "65bcbffe-868d-49a6-8837-c1f542d4d5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, the pandas library is imported to read the CSV file that is already present in the folder.\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "05c01fe2-327e-4085-9c86-8df9b75c6f1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Customer</th>\n",
       "      <th>Income (USD)</th>\n",
       "      <th>Age</th>\n",
       "      <th>Credit History (months)</th>\n",
       "      <th>Outstanding Debt (USD)</th>\n",
       "      <th>Defaulted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>541</th>\n",
       "      <td>542</td>\n",
       "      <td>70000</td>\n",
       "      <td>38.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>542</th>\n",
       "      <td>543</td>\n",
       "      <td>65000</td>\n",
       "      <td>33.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543</th>\n",
       "      <td>544</td>\n",
       "      <td>55000</td>\n",
       "      <td>29.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>544</th>\n",
       "      <td>545</td>\n",
       "      <td>48000</td>\n",
       "      <td>45.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>545</th>\n",
       "      <td>546</td>\n",
       "      <td>60000</td>\n",
       "      <td>55.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>546</th>\n",
       "      <td>547</td>\n",
       "      <td>35000</td>\n",
       "      <td>28.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>15000.0</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547</th>\n",
       "      <td>548</td>\n",
       "      <td>55000</td>\n",
       "      <td>31.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>548</th>\n",
       "      <td>549</td>\n",
       "      <td>80000</td>\n",
       "      <td>43.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549</th>\n",
       "      <td>550</td>\n",
       "      <td>60000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>May be</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>550</th>\n",
       "      <td>551</td>\n",
       "      <td>45000</td>\n",
       "      <td>31.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Customer  Income (USD)   Age  Credit History (months)  \\\n",
       "541       542         70000  38.0                     42.0   \n",
       "542       543         65000  33.0                     36.0   \n",
       "543       544         55000  29.0                     24.0   \n",
       "544       545         48000  45.0                     18.0   \n",
       "545       546         60000  55.0                     60.0   \n",
       "546       547         35000  28.0                     12.0   \n",
       "547       548         55000  31.0                     18.0   \n",
       "548       549         80000  43.0                     72.0   \n",
       "549       550         60000  50.0                     24.0   \n",
       "550       551         45000  31.0                     18.0   \n",
       "\n",
       "     Outstanding Debt (USD) Defaulted  \n",
       "541                  4000.0        No  \n",
       "542                  3000.0        No  \n",
       "543                  2000.0        No  \n",
       "544                  3000.0       Yes  \n",
       "545                  3000.0        No  \n",
       "546                 15000.0       Yes  \n",
       "547                  3000.0       Yes  \n",
       "548                  2000.0        No  \n",
       "549                  4000.0    May be  \n",
       "550                     NaN       NaN  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#In this step, the CSV file is read using the read_csv command, \n",
    "#and then we can examine the last 10 observations to get a sense of how the data looks.\n",
    "# One can also check first 10 observations by usine dp.head(10)\n",
    "\n",
    "df=pd.read_csv(\"PD_model_data.csv\")\n",
    "df.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "70ec3e0f-0d1c-41f3-9686-219b936d614c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 551 entries, 0 to 550\n",
      "Data columns (total 6 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   Customer                 551 non-null    int64  \n",
      " 1   Income (USD)             551 non-null    int64  \n",
      " 2   Age                      540 non-null    float64\n",
      " 3   Credit History (months)  545 non-null    float64\n",
      " 4   Outstanding Debt (USD)   546 non-null    float64\n",
      " 5   Defaulted                549 non-null    object \n",
      "dtypes: float64(3), int64(2), object(1)\n",
      "memory usage: 26.0+ KB\n"
     ]
    }
   ],
   "source": [
    "#It is to gain valuable insights into the dataset's structure, including its size, column data types, and potential missing values. \n",
    "#This information serves as a fundamental starting point for further data exploration and analysis tasks\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648aa310-810a-4061-8977-6698ad6b412a",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79671ebd-a252-49d0-9c36-132679f1f008",
   "metadata": {},
   "source": [
    "Based on the provided information, here is the summary:\n",
    "\n",
    "- There are a total of 551 observations with 6 columns.\n",
    "- There are no missing values in the \"Customer\" and \"Income\" variables.\n",
    "- There are 11 missing values in the \"Age\" variable, 6 missing values in the \"Credit History\" variable, 5 missing values in the \"Outstanding Debt\" variable, and 2 missing values in the \"Defaulted\" variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84c289d-13e4-4f1e-ada0-248a1e5890ac",
   "metadata": {},
   "source": [
    "## Step-2\n",
    "\n",
    "### Data Preprocessing: The bank preprocesses the data by handling missing values, converting categorical variables (such as defaulted status) into numerical representations, and scaling the numerical variables if necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93a9f07-b7ed-47b5-9d3b-efe81afb74ef",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c10fea-6206-45a1-9b69-a98109f8c208",
   "metadata": {},
   "source": [
    "#### Handling the missing value- \n",
    "When dealing with missing values in a PD (Probability of Default) model, there are several approaches one can consider. Here are a few common strategies:\n",
    "\n",
    "1- Complete Case Analysis: This approach involves excluding observations with missing values from the analysis. In our case, we could remove the rows with missing values for \"Age,\" \"Credit History,\" \"Outstanding Debt,\" and \"Defaulted.\" However, we have to keep in mind that this method may result in a loss of valuable data if the missing values are randomly distributed.\n",
    "\n",
    "2- Mean/Median/Mode Imputation: In this method, missing values are replaced with the mean, median, or mode value of the respective variable. For numerical variables like \"Age\" and \"Outstanding Debt,\" you can calculate the mean or median and substitute the missing values. For categorical variables like \"Credit History\" and \"Defaulted,\" you can replace the missing values with the mode (the most frequent value).\n",
    "\n",
    "3- Regression Imputation: If you have other variables that are strongly correlated with the missing variable, you can use regression models to estimate the missing values. For example, you can use a regression model with \"Income\" as the independent variable and \"Age\" as the dependent variable to predict missing \"Age\" values.\n",
    "\n",
    "4- Multiple Imputation: This method involves creating multiple imputations for missing values based on the observed data. It takes into account the uncertainty associated with missing values and produces multiple complete datasets for analysis.\n",
    "\n",
    "It is important to note that the choice of handling missing values depends on the specific characteristics of your data and the underlying assumptions of the PD model. It's essential to carefully consider the potential impact of each method on the model's accuracy and validity. Additionally, it's important to evaluate the potential bias introduced by the imputation methods and assess the robustness of your model's results.\n",
    "\n",
    "Remember, handling missing values is a crucial step in the data preprocessing phase of building a PD model, and the chosen approach should align with the specific requirements and goals of your analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "25f020ba-3cf7-4499-9b5a-9696c2e090f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 529 entries, 0 to 549\n",
      "Data columns (total 6 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   Customer                 529 non-null    int64  \n",
      " 1   Income (USD)             529 non-null    int64  \n",
      " 2   Age                      529 non-null    float64\n",
      " 3   Credit History (months)  529 non-null    float64\n",
      " 4   Outstanding Debt (USD)   529 non-null    float64\n",
      " 5   Defaulted                529 non-null    object \n",
      "dtypes: float64(3), int64(2), object(1)\n",
      "memory usage: 28.9+ KB\n"
     ]
    }
   ],
   "source": [
    "# Removing the missing line by using dropna()\n",
    "# Then check again if any missing value is there\n",
    "\n",
    "df2=df.dropna()\n",
    "df2.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342c8003-9631-43e1-bb23-3560d7454e7d",
   "metadata": {},
   "source": [
    "\n",
    "As can be seen, there is no missing value in df2 dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80622c9f-3449-484d-9725-c22e49e69676",
   "metadata": {},
   "source": [
    "#### Dealing with invalid data\n",
    "\n",
    "When dealing with invalid data in a PD (Probability of Default) model, it's important to identify and address these issues appropriately. Invalid data refers to observations that contain incorrect or nonsensical values that do not align with the expected range or format for a particular variable. Here are some common approaches for handling invalid data:\n",
    "\n",
    "- Data Cleaning: Review the data and identify any obvious errors or inconsistencies. If you notice extreme values or values that are outside the expected range for a variable, you may need to correct or remove those observations. For example, if you find negative values for \"Age\" or \"Income,\" it may indicate an error that needs to be addressed.\n",
    "\n",
    "- Outlier Detection and Treatment: Use statistical techniques to identify outliers in the data. Outliers are extreme values that deviate significantly from the majority of the data points. Depending on the nature of the outliers and their impact on the analysis, you can choose to remove them or transform them to more reasonable values.\n",
    "\n",
    "- Validation and Cross-Checking: Cross-check the data against external sources or perform additional validations to verify the accuracy and validity of the information. For example, you can verify employment status or income information by contacting the respective sources or using independent verification methods.\n",
    "\n",
    "- Expert Knowledge and Business Rules: Consult with domain experts or subject matter specialists who have a deep understanding of the data and the business context. They can provide valuable insights and guidance on how to handle specific invalid data scenarios based on their expertise.\n",
    "\n",
    "- Imputation or Replacement: In some cases, if the invalid data is missing or incomplete but can be reasonably estimated, you can use imputation techniques to replace the invalid values with plausible values based on other variables or statistical models. However, be cautious when imputing data and consider the potential impact on the model's results and validity.\n",
    "\n",
    "Remember, handling invalid data requires careful consideration and should be done in a manner that aligns with the specific characteristics of the data and the objectives of the PD model. It's crucial to maintain data integrity and ensure that any treatment applied to invalid data does not introduce bias or compromise the accuracy of the model's predictions.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ccf6d340-3576-4885-bed9-86177ff5c52e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range of Customer : ( 1 , 550 )\n",
      "range of Income (USD) : ( 35000 , 90000 )\n",
      "range of Age : ( -55.0 , 255.0 )\n",
      "range of Credit History (months) : ( 6.0 , 72.0 )\n",
      "range of Outstanding Debt (USD) : ( 2000.0 , 15000.0 )\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Iterate over the columns in the DataFrame\n",
    "for column in df2.columns:\n",
    "    # Check if the column is numeric\n",
    "    if df2[column].dtype in [int, float]:\n",
    "        # Print the range\n",
    "        print(\"range of\", column, \":\", \"(\", df2[column].min(), \",\",df2[column].max(),\")\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9d840dba-fa7d-47a3-8e7f-1b1ef4221c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct count of Defaulted column :\n",
      " No        332\n",
      "Yes       194\n",
      "May be      3\n",
      "Name: Defaulted, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Count the distinct values in a column\n",
    "distinct_counts = df2['Defaulted'].value_counts()\n",
    "# Print the count of distinct values\n",
    "print(\"Distinct count of Defaulted column :\\n\",distinct_counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44974b5d-f0d1-4e7a-a156-da48176e2192",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "From the above two analyses, we have discovered the following:\n",
    "\n",
    "- There are invalid values in the Age variable (Age < 0 and Age > 100).\n",
    "- There are invalid inputs in the Defaulted variable, indicated by the value \"May be\" (count - 3).\n",
    "\n",
    "Before making any decisions, we will count how many invalid values exist in the Age variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a1ac7935-9658-4f33-9a3a-175117c9be5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of invalid values in Age variable: 7\n"
     ]
    }
   ],
   "source": [
    "# Filter the DataFrame to get the invalid values in the \"Age\" variable\n",
    "invalid_age_count = len(df2[(df2['Age'] < 0) | (df2['Age'] > 100)])\n",
    "\n",
    "# Print the count of invalid values\n",
    "print(\"Count of invalid values in Age variable:\", invalid_age_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da4f91c-e443-4314-b721-7adf08641759",
   "metadata": {},
   "source": [
    "- Since these count are very less then we can delete these inputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "af6973e2-dc01-4056-8b6c-af281a969d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame excluding rows with invalid age inputs\n",
    "df_cleaned_age = df2[(df2['Age'] > 0) & (df2['Age'] < 100)].copy()\n",
    "\n",
    "# Reset the index of the new DataFrame\n",
    "df_cleaned_age.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9db09155-9719-48f2-ae49-3109571ed657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range of Customer : ( 1 , 550 )\n",
      "range of Income (USD) : ( 35000 , 90000 )\n",
      "range of Age : ( 27.0 , 55.0 )\n",
      "range of Credit History (months) : ( 6.0 , 72.0 )\n",
      "range of Outstanding Debt (USD) : ( 2000.0 , 15000.0 )\n"
     ]
    }
   ],
   "source": [
    "#To check the range of Age again\n",
    "# Iterate over the columns in the DataFrame\n",
    "for column in df_cleaned_age.columns:\n",
    "    # Check if the column is numeric\n",
    "    if df_cleaned_age[column].dtype in [int, float]:\n",
    "        # Print the range\n",
    "        print(\"range of\", column, \":\", \"(\", df_cleaned_age[column].min(), \",\",df_cleaned_age[column].max(),\")\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5192b2d7-6171-4171-948a-911ffe381b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deleting invalid input in Default variable\n",
    "\n",
    "# Create a new DataFrame excluding rows with invalid age inputs\n",
    "df_cleaned = df_cleaned_age[(df_cleaned_age['Defaulted']!= \"May be\")].copy()\n",
    "\n",
    "# Reset the index of the new DataFrame\n",
    "df_cleaned.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e0bbdcee-4317-4301-9fa5-63825f07d4cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct count of Defaulted column :\n",
      " No     325\n",
      "Yes    193\n",
      "Name: Defaulted, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# to check about invalid input on defaulted variable\n",
    "distinct_counts = df_cleaned['Defaulted'].value_counts()\n",
    "# Print the count of distinct values\n",
    "print(\"Distinct count of Defaulted column :\\n\",distinct_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "560647f7-2bc7-40da-b98a-d1943c389d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 518 entries, 0 to 517\n",
      "Data columns (total 6 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   Customer                 518 non-null    int64  \n",
      " 1   Income (USD)             518 non-null    int64  \n",
      " 2   Age                      518 non-null    float64\n",
      " 3   Credit History (months)  518 non-null    float64\n",
      " 4   Outstanding Debt (USD)   518 non-null    float64\n",
      " 5   Defaulted                518 non-null    object \n",
      "dtypes: float64(3), int64(2), object(1)\n",
      "memory usage: 24.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df_cleaned.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8f6530-1d04-4423-8809-cbb0f277e05a",
   "metadata": {},
   "source": [
    "Now we have a cleaned data which do not have any "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "af609e3e-bf4d-43a2-81bf-0b4a4fe079e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame df_modified based on df_cleaned\n",
    "df_modified = df_cleaned.copy()\n",
    "\n",
    "# Update values in the \"defaulted\" column based on conditions\n",
    "df_modified.loc[df_modified['Defaulted'] == \"Yes\", \"Defaulted\"] = 0\n",
    "df_modified.loc[df_modified['Defaulted'] == \"No\", \"Defaulted\"] = 1\n",
    "\n",
    "# Convert the \"defaulted\" column to integer type\n",
    "df_modified[\"Defaulted\"] = df_modified[\"Defaulted\"].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9ffe421d-fc04-41c2-bb62-48ca02668823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 518 entries, 0 to 517\n",
      "Data columns (total 6 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   Customer                 518 non-null    int64  \n",
      " 1   Income (USD)             518 non-null    int64  \n",
      " 2   Age                      518 non-null    float64\n",
      " 3   Credit History (months)  518 non-null    float64\n",
      " 4   Outstanding Debt (USD)   518 non-null    float64\n",
      " 5   Defaulted                518 non-null    int64  \n",
      "dtypes: float64(3), int64(3)\n",
      "memory usage: 24.4 KB\n"
     ]
    }
   ],
   "source": [
    "df_modified.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc07d7b-503c-4f50-a89e-40d1261a028d",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e6ae83-aeef-4f94-bc91-4aaf2aa34bd7",
   "metadata": {},
   "source": [
    "### Step3\n",
    "\n",
    "#### Feature selection, also known as variable selection, is the process of choosing a subset of relevant features or variables from a larger set of available features in a dataset. The goal of feature selection is to improve model performance, reduce overfitting, enhance interpretability, and decrease computational complexity by focusing on the most informative and influential features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c82260-d2f1-4f21-8856-652cbccecfab",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The process of feature selection typically involves the following steps:\n",
    "\n",
    "- Data Understanding: Gain a thorough understanding of the dataset, including the nature of the variables, their relationships, and their potential impact on the target variable. This step helps in identifying irrelevant or redundant variables.\n",
    "\n",
    "- Univariate Analysis: Conduct a univariate analysis by examining each feature individually and assessing its correlation or association with the target variable. Statistical tests, such as t-tests or chi-square tests, can be employed for this purpose. Features that exhibit a strong relationship with the target variable are generally considered more relevant.\n",
    "\n",
    "- Multivariate Analysis: Analyze the relationships among features themselves to identify potential redundancies or dependencies. This can involve techniques such as correlation analysis or variance inflation factor (VIF) calculation to identify highly correlated features. Redundant features may be removed as they provide redundant information.\n",
    "\n",
    "- Domain Knowledge and Expertise: Leverage domain knowledge or expert input to identify features that are known to be significant in the given problem domain. Expert insights can help prioritize certain variables based on their perceived importance.\n",
    "\n",
    "- Feature Ranking: Utilize various ranking methods to prioritize features based on their importance. Common techniques include:\n",
    "\n",
    "  -  Filter Methods: These methods assess the relevance of features independently of any specific machine learning model. They use statistical measures like correlation, mutual information, or chi-square tests to rank features and select the top-ranked ones.\n",
    "\n",
    "  - Wrapper Methods: Wrapper methods evaluate subsets of features by training and testing a machine learning model. They use the performance of the model as a criterion for feature selection, such as recursive feature elimination (RFE) or forward/backward selection.\n",
    "\n",
    "  - Embedded Methods: These methods incorporate feature selection within the model training process itself. They utilize built-in feature selection techniques available in certain machine learning algorithms, such as L1 regularization (Lasso) or tree-based feature importance.\n",
    "\n",
    "- Model Evaluation: Assess the performance of the model using the selected features. This evaluation provides insights into the impact of feature selection on the model's predictive ability and helps determine if further iterations of feature selection are necessary.\n",
    "\n",
    "- Iterative Process: Feature selection is often an iterative process where steps 2-6 are repeated multiple times to refine the feature set and improve model performance. Different techniques and combinations of features can be explored to identify the most effective subset.\n",
    "\n",
    "It's important to note that the specific feature selection techniques and steps may vary depending on the problem domain, the type of data, and the modeling approach being employed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e67d063-f3bf-4a37-a431-dd05bb4d75f1",
   "metadata": {},
   "source": [
    "#### In this example, no feature selection has been performed as all the independent variables are important for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090161c7-af82-48c6-8b46-0ab681906a13",
   "metadata": {},
   "source": [
    "## Step 4\n",
    "\n",
    "#### Splitting the Data: Split the dataset into a training set and a test set. For example, you can allocate 80% of the data for training and 20% for testing.\n",
    "\n",
    "#### Model Training: Train a PD model using a suitable algorithm such as logistic regression, random forest, or gradient boosting. Fit the model to the training data, using the features (income, age, credit history, outstanding debt) to predict the target variable (defaulted or not)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a4327a4e-9438-435f-88e4-e553a618c70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the Data\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and test sets\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(df_modified[['Income (USD)', 'Age', 'Credit History (months)', 'Outstanding Debt (USD)']], df_modified['Defaulted'], test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "57db48b9-db42-4db6-a640-e5f1a64ab39d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Income (USD)</th>\n",
       "      <th>Age</th>\n",
       "      <th>Credit History (months)</th>\n",
       "      <th>Outstanding Debt (USD)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>405</th>\n",
       "      <td>48000</td>\n",
       "      <td>45.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>3000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>60000</td>\n",
       "      <td>35.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>4000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>35000</td>\n",
       "      <td>28.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>15000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>45000</td>\n",
       "      <td>31.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>7000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>60000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>4000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Income (USD)   Age  Credit History (months)  Outstanding Debt (USD)\n",
       "405         48000  45.0                     18.0                  3000.0\n",
       "331         60000  35.0                     56.0                  4000.0\n",
       "220         35000  28.0                     12.0                 15000.0\n",
       "148         45000  31.0                     18.0                  7000.0\n",
       "301         60000  50.0                     24.0                  4000.0"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4ee27376-fff1-41d5-b275-97bf66b13d84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit the logistic regression model\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Create a logistic regression model object\n",
    "logreg_model = LogisticRegression()\n",
    "\n",
    "# Fit the model to the training data\n",
    "logreg_model.fit(train_data, train_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac2ed27-f952-45b5-bdd8-3f33b6ba9a43",
   "metadata": {},
   "source": [
    "## To evaluate of the performance of this model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352f4069-e844-41a5-a9bd-afd9551c4595",
   "metadata": {},
   "source": [
    "To evaluate the performance of the logistic regression model on the test set, you can use various evaluation metrics. Here are a few commonly used metrics for binary classification models:\n",
    "\n",
    "Accuracy: Accuracy measures the overall correctness of the model's predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4d706ba0-1f50-45cd-be95-04ee252cd62b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7980769230769231\n"
     ]
    }
   ],
   "source": [
    "# Evaluate accuracy\n",
    "accuracy = logreg_model.score(test_data, test_labels)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a901c1d9-9139-4b48-88c1-243f93c22491",
   "metadata": {},
   "source": [
    "- Confusion Matrix: A confusion matrix provides a detailed breakdown of the model's predictions by showing the true positive, true negative, false positive, and false negative counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5b554cdd-2485-4f4f-b47f-1bfe0b7575ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[23 20]\n",
      " [ 1 60]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Generate predictions on the test data\n",
    "predictions = logreg_model.predict(test_data)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(test_labels, predictions)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d732a9b-57b8-48a2-a0d3-88e4de564223",
   "metadata": {},
   "source": [
    "- Precision, Recall, and F1-Score: These metrics provide insights into the model's performance in terms of precision (ability to correctly identify positive instances), recall (ability to find all positive instances), and their harmonic mean F1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6b3952fb-57e2-4c9d-96ed-b67c6cc66c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.53      0.69        43\n",
      "           1       0.75      0.98      0.85        61\n",
      "\n",
      "    accuracy                           0.80       104\n",
      "   macro avg       0.85      0.76      0.77       104\n",
      "weighted avg       0.84      0.80      0.78       104\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Generate a classification report\n",
    "classification_rep = classification_report(test_labels, predictions)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_rep)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d43018-d272-4055-a563-05162e40c1e2",
   "metadata": {},
   "source": [
    "Based on the above evaluation metrics, the logistic regression model demonstrates a decent level of performance. Here's a breakdown of the metrics:\n",
    "\n",
    "- Accuracy: The accuracy of the model is 0.798, indicating that around 80% of the predictions on the test set are correct. However, accuracy alone may not provide a complete picture of the model's performance.\n",
    "\n",
    "- Confusion Matrix: The confusion matrix reveals the following:\n",
    "\n",
    "True Positives (TP): 60\n",
    "\n",
    "True Negatives (TN): 23\n",
    "\n",
    "False Positives (FP): 20\n",
    "\n",
    "False Negatives (FN): 1\n",
    "\n",
    "The model has a relatively high number of false positives (20) but only a single false negative (1).\n",
    "\n",
    "- Classification Report: The precision, recall, and F1-score for both classes (0 and 1) are as follows:\n",
    "\n",
    "Class 0: The precision is 0.96, indicating a high percentage of true negatives among the predicted negatives. The recall is 0.53, suggesting that the model may struggle to identify actual negatives. The F1-score is 0.69.\n",
    "Class 1: The precision is 0.75, indicating a good ability to identify true positives. The recall is 0.98, indicating that the model effectively captures most actual positives. The F1-score is 0.85.\n",
    "Considering these metrics, we can conclude the following justifications for the model's performance:\n",
    "\n",
    "The accuracy of 0.798 is reasonable, indicating that the model is making correct predictions for the majority of the instances in the test set.\n",
    "The high precision and recall for class 1 suggest that the model is effective at identifying actual defaults.\n",
    "However, the lower precision and recall for class 0 indicate that the model may struggle to correctly identify non-default cases, leading to a higher number of false positives.\n",
    "Overall, while the model performs well in detecting defaults (class 1), it could be further improved in distinguishing non-defaults (class 0). Depending on the specific requirements and the associated costs of false positives and false negatives, further model tuning or adjustments to the decision threshold may be necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b05c310-5cdb-4a12-b005-cfb99b248c20",
   "metadata": {},
   "source": [
    "\n",
    "### Lets try PD model with random forest approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b8297094-31a8-4b53-a97f-39f9cfeb4c72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create a Random Forest classifier object\n",
    "rf_model = RandomForestClassifier()\n",
    "\n",
    "# Fit the model to the training data\n",
    "rf_model.fit(train_data, train_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ca0e387c-f0a6-4f28-b9bd-dc951bdc5067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9903846153846154\n",
      "Confusion Matrix:\n",
      "[[43  0]\n",
      " [ 1 60]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99        43\n",
      "           1       1.00      0.98      0.99        61\n",
      "\n",
      "    accuracy                           0.99       104\n",
      "   macro avg       0.99      0.99      0.99       104\n",
      "weighted avg       0.99      0.99      0.99       104\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##checking the accuray of the modeL\n",
    "\n",
    "# Generate predictions on the test data\n",
    "rf_predictions = rf_model.predict(test_data)\n",
    "\n",
    "# Evaluate accuracy\n",
    "rf_accuracy = rf_model.score(test_data, test_labels)\n",
    "print(\"Accuracy:\", rf_accuracy)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "rf_conf_matrix = confusion_matrix(test_labels, rf_predictions)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(rf_conf_matrix)\n",
    "\n",
    "# Generate a classification report\n",
    "rf_classification_rep = classification_report(test_labels, rf_predictions)\n",
    "print(\"Classification Report:\")\n",
    "print(rf_classification_rep)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfe345d-6c5a-4ef9-a003-e0b66320c425",
   "metadata": {},
   "source": [
    "The Random Forest model demonstrates excellent performance on the test set, as indicated by the evaluation metrics:\n",
    "\n",
    "- Accuracy: The accuracy of the Random Forest model is 0.990, meaning that it correctly predicts the default status for approximately 99% of the instances in the test set.\n",
    "\n",
    "- Confusion Matrix: The model has 43 true positives, 60 true negatives, no false positives, and only one false negative. This suggests that the model has a very low rate of misclassifications.\n",
    "\n",
    "- Classification Report: The precision, recall, and F1-scores for both classes (0 and 1) are very high. Class 0 has a precision, recall, and F1-score of 0.98 and class 1 has a precision, recall, and F1-score of 1.00. This indicates that the Random Forest model performs exceptionally well in correctly identifying both defaults and non-defaults.\n",
    "\n",
    "#### In summary, the Random Forest model demonstrates significantly improved performance compared to the logistic regression model. With an accuracy of 0.990 and high precision, recall, and F1-scores for both classes, the Random Forest model is highly effective in predicting the default status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a3d07e6b-d3d3-48ef-b6db-fc33fdc6c8f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['random_forest_model.pkl']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#Save the Model:\n",
    "\n",
    "import joblib\n",
    "\n",
    "# Save the trained model\n",
    "joblib.dump(rf_model, 'random_forest_model.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "3f22ba28-0258-41ce-900f-dd91d26281e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: [1]\n"
     ]
    }
   ],
   "source": [
    "# Load the saved model\n",
    "loaded_model = joblib.load('random_forest_model.pkl')\n",
    "\n",
    "# Prepare new input data for prediction\n",
    "new_data = [[60000, 35, 30, 5000]]  # Example of new input data\n",
    "\n",
    "# Specify the feature names\n",
    "feature_names = ['Income (USD)', 'Age', 'Credit History (months)', 'Outstanding Debt (USD)']\n",
    "\n",
    "# Create a DataFrame for the new input data\n",
    "new_data_df = pd.DataFrame(new_data, columns=feature_names)\n",
    "\n",
    "# Use the loaded model to predict the result for new input data\n",
    "predictions = loaded_model.predict(new_data_df)\n",
    "\n",
    "# Print the predicted result\n",
    "print(\"Prediction:\", predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652639a9-9df3-454c-b812-5f356769704d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a0bdce-2018-4899-941b-d8651d78591a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
